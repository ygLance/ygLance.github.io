---
published: true
title: 对话系统论文笔记一：提高回复多样性及个性化对话系统
category: Dialogue System
tags: 
  - DL
  - DS
  - paper reading
layout: post
---

2019.10.24 开始尝试写对话系统的论文总结，每周都会更新，一周更新五篇论文左右。

2019.10.24 更新 Improving Neural Conversational Models with Entropy-Based Data Filtering

2019.10.28 更新 Generating Multiple Diverse Responses with Multi-Mapping and Posterior Mapping Selection

2019.10.28 更新 Personalized Dialogue Generation with Diversified Traits

2019.10.29 更新 Personalizing Dialogue Agents: I have a dog, do you have pets too?

# Improving Neural Conversational Models with Entropy-Based Data Filtering

ACL 2019

## 总结

基于seq2seq架构的对话系统存在*倾向于生成generic回复*的问题，论文作者认为这是因为对话系统的数据集中存在`one to many`和`many to one`的复杂关系，也就是说，在对话系统中存在一个`source utterance`对应多个不同含义`target utterance`的情况（反之亦然），这会给对话系统带来困扰：我该学习哪个target呢？基于这种状况，文章提出了一种方法来清洗数据集中熵（Entropy）较高的`source-target pair`，文章中的熵是这样定义的，如果一个source对应多个target，那么此source的熵较高（也可以用同样的方式来计算target的熵）。注意到这里指的不同target仅仅是在表面上不同，即*i dont know* 和*I don't know.*这两个句子虽然只有个别单词不一样，含义一样，但是也视为不同的target。 所以从直觉上来看，这里是需要做聚类的，若有一个source同时对应上述两个target，那么这两个target应被视为一样的答案。但从实验上来看，做聚类之后再清洗数据效果反而不如原来好，作者分析说可能是因为聚类的效果不太好。还有一点值得注意的是，从实验效果上来看，清洗熵较高的`target`效果比清洗`source`好。

## 一点思考

这种数据清洗的方法有后续工作可以跟进吗？这篇文章是将过于generic的pair清洗掉，那么我在做persona-dialogue或者style-dialogue的时候是不是也能通过类似的方式，*把不想要的数据清洗掉*，用简单的`one-to-one`的模型得到我想要的response呢？

## 一点吐槽

论文的思想非常简单，可以说methods也并不复杂，但为什么就是能中ACL呢？人家实验用了17个自动指标，非常充分且详细啊。而且如果让我来写的话，我肯定是不会说用熵来计算，而只会想到frequency之类的很low的东西。

# Generating Multiple Diverse Responses with Multi-Mapping and Posterior Mapping Selection

IJCAI 2019

## 总结

对话系统的输入和输出可看作是`one-to-many`的映射关系，有许多工作尝试用`latent machanism`去刻画这种映射关系，但是在作者看来，这种`latent machanism`仍然不能很好的捕捉`one-to-many`的映射，因为前者在`optimization`的时候还是把所有`response`不加区分的丢进模型里，这就会造成模型的`rough optimization`，作者认为每个`response`应该只优化一种`latent machanism`，即某种`latent machanism`应该由其最相关的某些`responses`来优化，所以作者提出了一种`posterior mapping selection`的机制，即先用这种机制进行前置的选择，选择`response`对应的`machanism`，再去优化这种特定的`machanism`，所以在每一次做`optimization`的过程中，只有被选择的`module`的参数会被更新。整个模型其实并没有太亮眼的地方，但是引入了 *Matching Loss* 去衡量`source-response pair`的相关性。

## 一点思考

这种方式不就相当于提前做clustering，然后将聚好的类丢进不同的模型吗？

## 后续工作

这篇论文的模型还没能解决的事情是将`latent machanism`显式的表示出来，虽然可以由不同的通道输出不同的回复，但是为什么这个通道能输出这种回复，这个通道输出的回复的特点是什么，类似于这样的问题它都没能回答，所以显而易见的，我们可以加入外部信息去指导每个通道生成的回复，这个外部信息是显式的，可以是persona，也可以是style,但是怎么引入外部知识呢？

# Personalized Dialogue Generation with Diversified Traits 

## 总结

这篇论文相当于在`speaker model`上做了extension，`speaker model`是将表示`speaker`的向量和`source`的`embedding`做拼接之后输入模型，这个表示`speaker`的向量是隐式的，而作者提出了一种能利用显式的人物traits的模型。那么这种traits如何在model中表达呢？这个问题分成两个部分，第一是traits如何表达，第二是如何在模型中体现。作者通过`Personality Trait Fusion` 将原本为 `key-value` 键值对的traits转换成vector，再通过`PAA`或`PAB`将性格vector引入模型，PAA就是在模型做attention时引入vector，PAB则是在模型做输出之前引入vector。本文还有一个突出贡献是构造了一个对话数据集，这个对话数据集`PersonalDialog`收集于微博，并且每段对话附有对话者的`profile`信息，此`profile`来自于其微博的个人资料，详情见文章。

# Personalizing Dialogue Agents: I have a dog, do you have pets too?

## 总结

这篇论文的贡献主要在在数据集的构建上，在个性化对话领域，数据集一直是个令人头疼的问题，因为带有人物`profile`或者`persona`信息的对话数据难以收集，上面有一篇文章也做了类似的工作，建立了一个带有对话双方`profile`的对话数据集`PersonalDialog`，但我翻了一下，发现`PersonalDialog`中的对话能跟其`profile`产生联系的比例非常之少，有很多的`profile`信息是无用的，跟对话毫不相关。这篇文章的数据集是在Amazon Mechanical Turk上花钱雇佣Turkers们人工构造的，并不是真实数据，所以可以肯定的是，这个数据集`persona-chat`的噪声肯定比`PersonalDialog`要小的多，并且还可以强制让对话的内容和事先设定的`persona`有关，事实上作者也是这么做的：先人工写好了`1155`个peronas，再给Turkers们随机分配persona，让他们扮演此persona进行聊天，以此收集对话数据。不过这样的数据集肯定不能完全拟合真实世界的对话数据，为了让数据集更多样更真实，作者也做了一系列措施，详情见文章。





