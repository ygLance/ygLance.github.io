---
published: true
title: 对话系统论文笔记六
category: Dialogue System
tags: 
  - DL
  - DS
  - paper reading
layout: post
---

2019.11.22 更新 Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders

2019.11.26 更新 Learning from Dialogue after Deployment: Feed Yourself, Chatbot!

2019.12.3 更新 Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study

2019.12.5 更新 Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems

2019.12.5 更新 Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems

# Generating Multiple Diverse Responses for Short-Text Conversation

AAAI 2019

## 总结

# Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders

ACL 2017

## 总结

这篇将CVAE引入了对话的seq2seq架构，具体做法是，context $c$ 和 response $x$ 通过 Recognition Network生成 $z$，context $c$ 单独通过 Prior Network生成 $z^{'}$，通过KL散度使这两者的分布尽量相同，从而得到了一个更好的编码器，并且我们能通过在 $z$ 的分布中采样，输入decoder中得到不同回复。

# Learning from Dialogue after Deployment: Feed Yourself, Chatbot!

ACL 2019

## 总结

算是别出心裁的一个工作，但是还有地方仍需可解释性。文章的motivation是：人类在学习如何对话时，不仅仅参考别人之间的对话，而且还在自己和他人对话后接受对方的反馈，提升自身对话本领。所以，如果我们能在agent与human对话后，获取user对当前response的feedback，再通过feedback调整模型，是不是能够提高模型的性能呢？作者先用`PersonaChat`数据集训练好一个对话agent，再通过supervised learning人工标注sentiment label后训练一个`satisfaction predictor`，如果判断user满意agent的response，则将user的response作为新数据收集起来，如果判断user不满意agent的response,比如" What are you talking about?"，则通过提问题的方式获取user的feedback，将feedback作为新数据收集起来，新数据+旧数据重新训练，模型性能有了明显提升。有一点需要提的是，此model不管feedback的类型和内容如何，均按照一样的处理方式，作者相信feedback和context之间存在联系。这种暴力的直接将feedback丢进去model的方式肯定是有待商榷的，文章没能说服我。

# Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study

ACL 2019

## 总结

这个工作挺有意思的，在utterance-level和word-level各自添加perturbations，然后查看model的PPL变动情况，从而研究model是否有效的利用了对话历史，以及对于这种perturbations是否足够敏感，然而，这里有一个问题是，若model对perturbations不敏感，就能说明模型不够powerful，或者能说明模型没有充分利用信息吗？以word-level为例，当一个句子的词语的语序发生变化，model对前后两个句子的回复是相同的，有没有这么一种可能：模型太强了，把乱序句子中潜在的information都学到了，人的确是有这种能力的，“研表究明，汉字的序顺并不定一能影阅响读”，比如这句话，大脑会自动按照记忆中的顺序，自动对其排序，这是大脑功能强大的一种体现。所以，其实是需要进一步做实验，如果一句话的词语语序不同，表达的意思不同，那么对其生成的回复是否是一样的？做这样的实验或许能够进一步说明问题。

# Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems

EMNLP 2017

## 总结

2017年的工作，看这篇的原因是很多文章都会用这个作者提出的技术引入外部信息。文章主要是讲如何将cue word引入，作者设计了一个cue word GRU，原来的GRU的输入之一是上一个词的word embedding，cue word GRU则变成是cue word的word embedding，然后用一个fusion unit将两个GRU的hidden state融合。这篇文章的motivation对我现在在做的一个工作有一定参考价值，作者认为以前引入cue word帮助生成回复，这个cue word必须显式的出现在response中，作者认为这样会造成一定程度上的rigid，生成跟cue word语义相关的句子是一个更好的选择，品品文中这句话：

> In this way, we change the “hard” content-introducing method into a new “soft” schema.

# Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset

ACL 2019

## 总结

提出了一个新任务，如何让chatbot变得善解人意？基于数据训练出来的chatbot可能会很aggressive，或是理解不了对方谈话时的心情，比如在对话时，察觉到对方afraid/proud/faithful等情绪，应该产生对应的response，afraid就应该给予对方帮助，察觉到对面此时是proud就应该称赞等等。其实这个motivation我还觉得挺make sense的，这篇文章就基于此，提出了Empathetic的数据集和基于这个数据集的baseline。